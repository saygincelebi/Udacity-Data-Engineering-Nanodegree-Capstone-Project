{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Summary\n",
    "This project aims to have an end-to-end data pipeline for the US Immigration Services in order them to utilize their immigration data combined with demographics and weather based information provided from different sources.\n",
    "\n",
    "The ETL pipeline has 3 main phases, that will be explained in detail\n",
    "\n",
    "- Data Gathering (Extraction)\n",
    "- Data Exploration, Cleaning & Joins (Transformation)\n",
    "- Data Ingestion (Loading)\n",
    "\n",
    "Contents:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#declare a config file to get AWS credentials\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Initialize Spark, with additional SAS libraries\n",
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\n",
    "                                        \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Project Scope\n",
    "\n",
    "This project aims to have an end-to-end data pipeline for the US Immigration Services in order them to utilize their immigration data combined with demographics and weather based information provided from different sources. By this way, the Business Intelligence and Data Analytics layers that are fed by the DWH would be able to create insights from immigration data enriched with weather, demographic and time information. \n",
    "- Some example insights would be like:\n",
    "\t - The correlations of temperature data between US States/Cities and the immigrant countries. (Do people coming from warm weather prefer to immigrate to warm climates?)\n",
    "\t - Time series analysis of target and destination cities based on climate.\n",
    "\t - The effect of temperature on immigrated states? (number of immigrations, number of countries immigrated from etc.)\n",
    "\t - Some predictional models can be used to forecast the immigration numbers in future. Example; The relation between temperature and population density of cities in time can be used to predict the effect populations to immigrate from very hot to mild weather states/cities due to global warming.\n",
    "\n",
    "The ETL pipeline that is is scheduled on Airflow has 3 main phases;\n",
    "\n",
    "- **Data Gathering (Extraction)**: I assumed the data consisting from 4 different source files resides on different sources, so I uploaded 1 source file to Amazon S3 and kept 3 on Udacity servers. The extraction phase runs in parallel and fetches its source data at the same time frame.  The data model has 4 dimension tables and 1 fact table. So as in total, there are 5 seperate batch jobs. At first, four of these process', which will be used to load the dimension dataframes are triggered from Airflow. Each Spark job loads the necessary file and creates it's dataframe, which consists of uncleaned raw data. (The raw data in the dataframes will be cleaned at first and then will be converted to parquet files and these files will be loaded into tables)\n",
    "\n",
    "- **Data Exploration, Cleaning & Joins (Transformation)**:  After the 4 dimension data frames are loaded as dataframes (raw data), the data is analysed and some cleaning steps are taken. The exploration and cleaning steps are explained in more detail in the code as comments. After the cleaning steps are completed, these 4 dataframes are loaded into Amazon S3 buckets as parquet files. The fact table process starts after the dimension steps are completed. The data quality checks are made at the end of each seperate process. (As a difference, the quality check step in the .ipynb code is performed as the final step for all of the dataframes created)\n",
    "\n",
    "[![End-to-End Flow](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/ETL_flow.png?raw=true \"End-to-End Flow\")](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/ETL_flow.png?raw=true \"End-to-End Flow\")\n",
    "\n",
    "###### *The first 4 process are executed 5 times. 4 times for dimensions (in parallel) and one for the fact table. After the files are loaded into S3 buckets, the last process is also executed 5 times. 4 times for dimensions (in parallel) and one for the fact table.*\n",
    "\n",
    "- **Data Ingestion (Loading)**: In the last phase, all of the dimension and fact Parquet files residing on the S3 buckets are ingested into the Redshift tables. Similar to the previous steps, the dimension tables are loaded in parallel and the fact table is loaded after all the dimensions are loaded. Instead of using Spark ,this step is direct executed from Airflow using the StageToRedshiftOperator. \n",
    "\n",
    "[![Airflow - DAG](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/Airflow%20-%20Dag.PNG?raw=true \"Airflow - DAG: ETL_Flow\")](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/Airflow%20-%20Dag.PNG?raw=true \"Airflow - DAG\")\n",
    "\n",
    "###### *The gray  boxes are bash operators that run the Spark scripts in order to load the files to S3, the green boxes are StagetoRedshift operators that are used to ingest these files to Redshift.*\n",
    "\n",
    "#### The Datasets\n",
    "\n",
    "The project has 4 main data sources. I assumed there are two different physical data sources. One is the Udacity servers, the other is the Amazon S3 servers. One of the goals of the project is to combine the data gathered from different data sources.\n",
    "\n",
    "- **US Immigration Data**\n",
    "\t- This data is provided by the US National Tourism and Trade Office.  (https://travel.trade.gov/research/reports/i94/historical/2016.html) A data dictionary is included in the workspace. For this project a partition containing a month of historical data is used.  (~9.3 million records) \n",
    "\t- The data is ingested from the Udacity servers during the flow. (Location: ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat)\n",
    "\n",
    "- **U.S. City Demographic Data**\n",
    "\t- This data is provided by the US Census Bureau's 2015 American Community Survey.  (https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/) This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. (~3000 records) \n",
    "\t- The data is ingested from the Amazon S3 servers during the flow.\n",
    "\n",
    "- **World Temperature Data**\n",
    "\t- This data is provided by the  Berkeley Earth Surface Temperature Study.  (https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) It combines 1.6 billion temperature reports from 16 pre-existing archives. (~8.6 million records) It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included, meaning fewer observations need to be thrown away.\n",
    "\t- The data is ingested from the Udacity servers during the flow.  (Location: ../../data2/GlobalLandTemperaturesByCity.csv)\n",
    "\n",
    "- **Country Codes**\n",
    "\t- This data is based on the US Immigration data provided by US National Tourism and Trade Office.  (https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) It has 3 digit country codes for 289 countries (Including Non Country Codes)\n",
    "\t- The data is ingested from the Udacity servers during the flow. The original file is the ../../data/I94_SAS_Labels_Descriptions.SAS file but I have provided a simplified .csv version that only contains the country codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Reading the immigration dataset (Data resides on Udacity servers)\n",
    "file_name = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_immigration =spark.read.format('com.github.saurfang.sas.spark').load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>None</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247104e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247140e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247161e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247080e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20562.0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247849e+10</td>\n",
       "      <td>00608</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN    None   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "5   18.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MI   \n",
       "6   19.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NJ   \n",
       "7   20.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NJ   \n",
       "8   21.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NY   \n",
       "9   22.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NY   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U     None   1979.0  10282016   None   None   \n",
       "1      NaN   ...           Y     None   1991.0       D/S      M   None   \n",
       "2  20691.0   ...        None        M   1961.0  09302016      M   None   \n",
       "3  20567.0   ...        None        M   1988.0  09302016   None   None   \n",
       "4  20567.0   ...        None        M   2012.0  09302016   None   None   \n",
       "5  20555.0   ...        None        M   1959.0  09302016   None   None   \n",
       "6  20558.0   ...        None        M   1953.0  09302016   None   None   \n",
       "7  20558.0   ...        None        M   1959.0  09302016   None   None   \n",
       "8  20553.0   ...        None        M   1970.0  09302016   None   None   \n",
       "9  20562.0   ...        None        M   1968.0  09302016   None   None   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0    None  1.897628e+09   None       B2  \n",
       "1    None  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "5      AZ  9.247104e+10  00602       B1  \n",
       "6      AZ  9.247140e+10  00602       B2  \n",
       "7      AZ  9.247161e+10  00602       B2  \n",
       "8      AZ  9.247080e+10  00602       B2  \n",
       "9      AZ  9.247849e+10  00608       B1  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_immigration.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Reading the Global Land Temperatures by City dataset #Reading the immigration dataset (Data resides on Udacity servers)\n",
    "file_name = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.csv(file_name, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "5 1744-04-01               5.788                          3.624  Århus   \n",
       "6 1744-05-01              10.644                          1.283  Århus   \n",
       "7 1744-06-01              14.051                          1.347  Århus   \n",
       "8 1744-07-01              16.082                          1.396  Århus   \n",
       "9 1744-08-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  \n",
       "5  Denmark   57.05N    10.33E  \n",
       "6  Denmark   57.05N    10.33E  \n",
       "7  Denmark   57.05N    10.33E  \n",
       "8  Denmark   57.05N    10.33E  \n",
       "9  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_temperature.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Reading the immigration dataset (Data resides on Amazon S3 servers)\n",
    "file_name = \"s3a://celebis/source/demographics/us-cities-demographics.csv\"\n",
    "df_demographics = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229</td>\n",
       "      <td>62432</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634</td>\n",
       "      <td>7517</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712</td>\n",
       "      <td>41971</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815</td>\n",
       "      <td>8355</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629</td>\n",
       "      <td>56860</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800</td>\n",
       "      <td>37038</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762</td>\n",
       "      <td>43270</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783</td>\n",
       "      <td>3269</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751</td>\n",
       "      <td>58077</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204</td>\n",
       "      <td>16315</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City           State  Median Age  Male Population  \\\n",
       "0     Silver Spring        Maryland        33.8            40601   \n",
       "1            Quincy   Massachusetts        41.0            44129   \n",
       "2            Hoover         Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga      California        34.5            88127   \n",
       "4            Newark      New Jersey        34.6           138040   \n",
       "5            Peoria        Illinois        33.1            56229   \n",
       "6          Avondale         Arizona        29.1            38712   \n",
       "7       West Covina      California        39.8            51629   \n",
       "8          O'Fallon        Missouri        36.0            41762   \n",
       "9        High Point  North Carolina        35.5            51751   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "5              62432            118661                6634          7517   \n",
       "6              41971             80683                4815          8355   \n",
       "7              56860            108489                3800         37038   \n",
       "8              43270             85032                5783          3269   \n",
       "9              58077            109828                5204         16315   \n",
       "\n",
       "   Average Household Size State Code                               Race  Count  \n",
       "0                    2.60         MD                 Hispanic or Latino  25924  \n",
       "1                    2.39         MA                              White  58723  \n",
       "2                    2.58         AL                              Asian   4759  \n",
       "3                    3.18         CA          Black or African-American  24437  \n",
       "4                    2.73         NJ                              White  76402  \n",
       "5                    2.40         IL  American Indian and Alaska Native   1343  \n",
       "6                    3.18         AZ          Black or African-American  11592  \n",
       "7                    3.56         CA                              Asian  32716  \n",
       "8                    2.77         MO                 Hispanic or Latino   2583  \n",
       "9                    2.65         NC                              Asian  11060  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Checking the data\n",
    "df_demographics.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploring and Assessing the Data\n",
    "For each dataframe the exploration and cleaning steps are different.\n",
    "Each of these steps are documented in comments.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Cleaning steps are also documented in comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Immigration Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the number of nan and null rows for each column\n",
    "df_count_nan = df_immigration.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_immigration.columns]).toPandas()\n",
    "    \n",
    "#converting the dataframe to long format from wide format\n",
    "df_count_nan = pd.melt(df_count_nan, var_name='columns', value_name='values')\n",
    "\n",
    "#counting total number of records in the dataframe\n",
    "no_of_recs = df_immigration.count()\n",
    "    \n",
    "#finding the percentage of missing values for each column\n",
    "df_count_nan['% missing values'] = 100*df_count_nan['values']/no_of_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>values</th>\n",
       "      <th>% missing values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cicid</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i94yr</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i94mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i94cit</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i94res</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i94port</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>arrdate</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i94mode</td>\n",
       "      <td>239</td>\n",
       "      <td>0.007719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i94addr</td>\n",
       "      <td>152592</td>\n",
       "      <td>4.928184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>depdate</td>\n",
       "      <td>142457</td>\n",
       "      <td>4.600859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i94bir</td>\n",
       "      <td>802</td>\n",
       "      <td>0.025902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i94visa</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dtadfile</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>visapost</td>\n",
       "      <td>1881250</td>\n",
       "      <td>60.757746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>occup</td>\n",
       "      <td>3088187</td>\n",
       "      <td>99.737559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>entdepa</td>\n",
       "      <td>238</td>\n",
       "      <td>0.007687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>entdepd</td>\n",
       "      <td>138429</td>\n",
       "      <td>4.470769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>entdepu</td>\n",
       "      <td>3095921</td>\n",
       "      <td>99.987340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>matflag</td>\n",
       "      <td>138429</td>\n",
       "      <td>4.470769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>biryear</td>\n",
       "      <td>802</td>\n",
       "      <td>0.025902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dtaddto</td>\n",
       "      <td>477</td>\n",
       "      <td>0.015405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>gender</td>\n",
       "      <td>414269</td>\n",
       "      <td>13.379429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>insnum</td>\n",
       "      <td>2982605</td>\n",
       "      <td>96.327632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>airline</td>\n",
       "      <td>83627</td>\n",
       "      <td>2.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>admnum</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fltno</td>\n",
       "      <td>19549</td>\n",
       "      <td>0.631364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>visatype</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     columns   values  % missing values\n",
       "0      cicid        0          0.000000\n",
       "1      i94yr        0          0.000000\n",
       "2     i94mon        0          0.000000\n",
       "3     i94cit        0          0.000000\n",
       "4     i94res        0          0.000000\n",
       "5    i94port        0          0.000000\n",
       "6    arrdate        0          0.000000\n",
       "7    i94mode      239          0.007719\n",
       "8    i94addr   152592          4.928184\n",
       "9    depdate   142457          4.600859\n",
       "10    i94bir      802          0.025902\n",
       "11   i94visa        0          0.000000\n",
       "12     count        0          0.000000\n",
       "13  dtadfile        1          0.000032\n",
       "14  visapost  1881250         60.757746\n",
       "15     occup  3088187         99.737559\n",
       "16   entdepa      238          0.007687\n",
       "17   entdepd   138429          4.470769\n",
       "18   entdepu  3095921         99.987340\n",
       "19   matflag   138429          4.470769\n",
       "20   biryear      802          0.025902\n",
       "21   dtaddto      477          0.015405\n",
       "22    gender   414269         13.379429\n",
       "23    insnum  2982605         96.327632\n",
       "24   airline    83627          2.700857\n",
       "25    admnum        0          0.000000\n",
       "26     fltno    19549          0.631364\n",
       "27  visatype        0          0.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the % of missing values for each column\n",
    "df_count_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the columns that have %90 or more missing values \n",
    "missing_vcolumns=list(df_count_nan.loc[df_count_nan['% missing values'] >= 90]['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['occup', 'entdepu', 'insnum']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying these columns, if there are any\n",
    "missing_vcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping these columns\n",
    "df_immigration = df_immigration.drop(*missing_vcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Assuming the 'cicid' column is the primary key, we drop duplicate entries, if there are any\n",
    "df_immigration = df_immigration.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Assuming the 'cicid' column is the primary key, we drop these entries with null values, if there are any\n",
    "df_immigration = df_immigration.dropna(how='all', subset=['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop entries with all null values, if there are any\n",
    "df_immigration = df_immigration.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,096,313\n"
     ]
    }
   ],
   "source": [
    "#Checking the number of rows after dropping missing values\n",
    "count_df_immigration= df_immigration.count()\n",
    "print('{:,}'.format(count_df_immigration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Temperature Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Converting date column type to string\n",
    "df_temperature = df_temperature.withColumn(\"dt\",col(\"dt\").cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the number of nan and null rows\n",
    "df_count_nan = df_temperature.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_temperature.columns]).toPandas()\n",
    "    \n",
    "#converting the dataframe to long format from wide format\n",
    "df_count_nan = pd.melt(df_count_nan, var_name='columns', value_name='values')\n",
    "    \n",
    "#counting total number of records in the dataframe\n",
    "no_of_recs = df_temperature.count()\n",
    "    \n",
    "#finding the percentage of missing values for each column\n",
    "df_count_nan['% missing values'] = 100*df_count_nan['values']/no_of_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>values</th>\n",
       "      <th>% missing values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AverageTemperature</td>\n",
       "      <td>364130</td>\n",
       "      <td>4.234458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AverageTemperatureUncertainty</td>\n",
       "      <td>364130</td>\n",
       "      <td>4.234458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         columns  values  % missing values\n",
       "0                             dt       0          0.000000\n",
       "1             AverageTemperature  364130          4.234458\n",
       "2  AverageTemperatureUncertainty  364130          4.234458\n",
       "3                           City       0          0.000000\n",
       "4                        Country       0          0.000000\n",
       "5                       Latitude       0          0.000000\n",
       "6                      Longitude       0          0.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the % of missing values for each column\n",
    "df_count_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the columns that have %90 or more missing values\n",
    "missing_vcolumns=list(df_count_nan.loc[df_count_nan['% missing values'] >= 90]['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying these columns, if there are any\n",
    "missing_vcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping these columns\n",
    "df_temperature = df_temperature.drop(*missing_vcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop these entries with null values, if there are any\n",
    "df_temperature = df_temperature.dropna(subset=['AverageTemperature','AverageTemperatureUncertainty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop entries with duplicate values, if there are any (we need one temperature record per city, country & date)\n",
    "df_temperature = df_temperature.drop_duplicates(subset=['dt', 'City', 'Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop entries with all null values, if there are any\n",
    "df_temperature = df_temperature.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,190,783\n"
     ]
    }
   ],
   "source": [
    "#Checking the number of rows after dropping missing values\n",
    "count_df_temperature= df_temperature.count()\n",
    "print('{:,}'.format(count_df_temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the number of nan and null rows for each column\n",
    "df_count_nan = df_demographics.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_demographics.columns]).toPandas()\n",
    "    \n",
    "#converting the dataframe to long format from wide format\n",
    "df_count_nan = pd.melt(df_count_nan, var_name='columns', value_name='values')\n",
    "    \n",
    "#counting total number of records in the dataframe\n",
    "no_of_recs = df_demographics.count()\n",
    "    \n",
    "#finding the percentage of missing values for each column\n",
    "df_count_nan['% missing values'] = 100*df_count_nan['values']/no_of_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>values</th>\n",
       "      <th>% missing values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Median Age</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total Population</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Veterans</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Foreign-born</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Average Household Size</td>\n",
       "      <td>16</td>\n",
       "      <td>0.553442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>State Code</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Race</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   columns  values  % missing values\n",
       "0                     City       0          0.000000\n",
       "1                    State       0          0.000000\n",
       "2               Median Age       0          0.000000\n",
       "3          Male Population       3          0.103770\n",
       "4        Female Population       3          0.103770\n",
       "5         Total Population       0          0.000000\n",
       "6       Number of Veterans      13          0.449671\n",
       "7             Foreign-born      13          0.449671\n",
       "8   Average Household Size      16          0.553442\n",
       "9               State Code       0          0.000000\n",
       "10                    Race       0          0.000000\n",
       "11                   Count       0          0.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the % of missing values for each column\n",
    "df_count_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the columns that have %90 or more missing values\n",
    "missing_vcolumns=list(df_count_nan.loc[df_count_nan['% missing values'] > 90]['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying these columns, if there are any\n",
    "missing_vcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping these columns\n",
    "df_demographics = df_demographics.drop(*missing_vcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#finding the columns that have missing values\n",
    "missing_vcolumns=list(df_count_nan.loc[df_count_nan['% missing values'] > 0]['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Male Population',\n",
       " 'Female Population',\n",
       " 'Number of Veterans',\n",
       " 'Foreign-born',\n",
       " 'Average Household Size']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying these columns\n",
    "missing_vcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop these entries with null values\n",
    "df_demographics = df_demographics.dropna(subset=missing_vcolumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Assuming the 'City', 'State', 'State Code', 'Race' columns make the primary key, we drop these entries with null values, if there are any\n",
    "df_demographics= df_demographics.drop_duplicates(subset=['City', 'State', 'State Code', 'Race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#We drop entries with all null values, if there are any\n",
    "df_demographics = df_demographics.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,875\n"
     ]
    }
   ],
   "source": [
    "#Checking the number of rows after dropping missing values\n",
    "count_df_demographics= df_demographics.count()\n",
    "print('{:,}'.format(count_df_demographics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary for Immigration Data</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">cicid</td><td class=\"tg-0pky\">Unique record ID</td>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td>\n",
    " <tr><td class=\"tg-0pky\">i94cit</td><td class=\"tg-0pky\">3 digit code for immigrant country of birth</td>\n",
    " <tr><td class=\"tg-0pky\">i94res</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td>\n",
    " <tr><td class=\"tg-0pky\">insnum</td><td class=\"tg-0pky\">INS number</td>\n",
    " <tr><td class=\"tg-0pky\">airline</td><td class=\"tg-0pky\">Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">admnum</td><td class=\"tg-0pky\">Admission Number</td>\n",
    " <tr><td class=\"tg-0pky\">fltno</td><td class=\"tg-0pky\">Flight number of Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">visatype</td><td class=\"tg-0pky\">Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary for Temperature Data</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">dt</td><td class=\"tg-0pky\">Date</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperature</td><td class=\"tg-0pky\">Global average land temperature in celsius</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperatureUncertainty</td><td class=\"tg-0pky\">95% confidence interval around the average</td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">Name of City</td>\n",
    " <tr><td class=\"tg-0pky\">Country</td><td class=\"tg-0pky\">Name of Country</td>\n",
    " <tr><td class=\"tg-0pky\">Latitude</td><td class=\"tg-0pky\">City Latitude</td>\n",
    " <tr><td class=\"tg-0pky\">Longitude</td><td class=\"tg-0pky\">City Longitude</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary for Demographics Data</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">State Code</td><td class=\"tg-0pky\">Code of the US state</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: The Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "A Star Schema has been created in order to provide the reporing and analytics features described in Step 1.\n",
    "\n",
    "- **dim_calendar:** Time dimension table (Load Type: Truncate Insert)\n",
    "- **dim_country:** Country & average temperature dimension table (Load Type: Truncate Insert)\n",
    "- **dim_visatype:** Visa type dimension table (Load Type: Truncate Insert)\n",
    "- **dim_demographics:**  Demographics dimension table (Load Type: Truncate Insert)\n",
    "- **fact_immigration**: Immigration fact table (Load Type: Insert Append)\n",
    "\n",
    "The PK, FK, Indexes and Relations are described in more detail here: [Logical Data Model](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/Logical%20Data%20Model.txt \"Logical Data Model\")\n",
    "\n",
    "[![ER Diagram](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/ER%20Diagram.png?raw=true \"ER Diagram\")](https://github.com/saygincelebi/Udacity-Data-Engineering-Nanodegree-Capstone-Project/blob/main/diagrams/ER%20Diagram.png?raw=true \"ER Diagram\")\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "As described in Step 1, the data pipeline is as follows:\n",
    "\n",
    "1.  Loading files into dataframes (raw data)\n",
    "2.  Analyzing dataframes\n",
    "3.  Cleaning dataframes\n",
    "4.  Checking dataframes\n",
    "5.  Ingesting dim and fact data as parquet files on S3 buckets\n",
    "6.  Writing files to Redshift tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Running the Pipelines to Model the Data \n",
    "#### 4.1 Creating the data model\n",
    "The functions that will be creating the files on S3 buckets that will be loaded into the Star Schema on Redshift are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Type convertion for Numeric and String columns in order to prevent errors while loading data into Redshift\n",
    "def cast_type(df, cols):\n",
    "    \"\"\"\n",
    "    in: df: spark dataframe\n",
    "    in: cols: columns and types to be casted\n",
    "    out: spark dataframe with casted types\n",
    "    \"\"\"\n",
    "    for x,y in cols.items():\n",
    "        if x in df.columns:\n",
    "            df = df.withColumn(x, df[x].cast(y))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Saving the dataframe to S3 (or another file system)\n",
    "def save_df(df, output_path, mode = \"overwrite\", output_format = \"parquet\", columns = '*', partitionBy=None, **options):\n",
    "    \"\"\"\n",
    "    in: df: spark dataframe to be written into files\n",
    "    in: output_path: path to be written\n",
    "    in: parameters\n",
    "    \"\"\"\n",
    "    df.select(columns).write.save(output_path, mode= mode, format=output_format, partitionBy = partitionBy, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#S3 output path for dimension files\n",
    "dim_output_data = \"s3a://celebis/dimensions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#S3 output path for fact files\n",
    "fact_output_data = \"s3a://celebis/facts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function creates an immigration calendar dataframe based on arrival date\n",
    "def create_dim_calendar(df, dim_output_data):\n",
    "    \"\"\"\n",
    "    in:df: spark dataframe of immigration events\n",
    "    in:dim_output_data: path to write dimension dataframe to\n",
    "    out: spark dataframe calendar dimension\n",
    "    \"\"\"\n",
    "    #Creating a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "\n",
    "    #Creating initial calendar df from arrdate column\n",
    "    dim_calendar_df = df.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df.arrdate)).distinct()\n",
    "\n",
    "    #Expanding df by adding other calendar columns\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('arrival_month', month('arrdate'))\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('arrival_year', year('arrdate'))\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "\n",
    "    #Creating an id field in calendar df\n",
    "    dim_calendar_df = dim_calendar_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    integer_cols = ['arrival_day','arrival_week','arrival_month','arrival_year','arrival_weekday']\n",
    "\n",
    "    #Type conversion for integer columns  \n",
    "    calendar_df = cast_type(dim_calendar_df, dict(zip(integer_cols, len(integer_cols)*[IntegerType()])))\n",
    " \n",
    "    varchar_cols = ['id']\n",
    "    #Type conversion for varchar columns\n",
    "    dim_calendar_df = cast_type(calendar_df, dict(zip(varchar_cols, len(varchar_cols)*[StringType()])))\n",
    "    \n",
    "    #Writing the calendar dimension to parquet file\n",
    "    dim_calendar_df.write.parquet(dim_output_data + \"dim_calendar\", mode=\"overwrite\")\n",
    "\n",
    "    return dim_calendar_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the df_dim_calendar dataframe and writing the files to S3\n",
    "df_dim_calendar = create_dim_calendar(df_immigration, dim_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- arrival_day: integer (nullable = true)\n",
      " |-- arrival_week: integer (nullable = true)\n",
      " |-- arrival_month: integer (nullable = true)\n",
      " |-- arrival_year: integer (nullable = true)\n",
      " |-- arrival_weekday: integer (nullable = true)\n",
      " |-- id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema\n",
    "df_dim_calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrdate</th>\n",
       "      <th>arrival_day</th>\n",
       "      <th>arrival_week</th>\n",
       "      <th>arrival_month</th>\n",
       "      <th>arrival_year</th>\n",
       "      <th>arrival_weekday</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>8589934592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>25769803776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-18</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>42949672960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04-09</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>7</td>\n",
       "      <td>68719476736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>85899345920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      arrdate  arrival_day  arrival_week  arrival_month  arrival_year  \\\n",
       "0  2016-04-22           22            16              4          2016   \n",
       "1  2016-04-15           15            15              4          2016   \n",
       "2  2016-04-18           18            16              4          2016   \n",
       "3  2016-04-09            9            14              4          2016   \n",
       "4  2016-04-11           11            15              4          2016   \n",
       "\n",
       "   arrival_weekday           id  \n",
       "0                6   8589934592  \n",
       "1                6  25769803776  \n",
       "2                2  42949672960  \n",
       "3                7  68719476736  \n",
       "4                2  85899345920  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_dim_calendar.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function calculates average temperature data for a given country\n",
    "def aggregate_temperature_data(df):\n",
    "    \"\"\"    \n",
    "    in: df: spark dataframe of global temperature data\n",
    "    out: spark dataframe consisting of countries average temperatures\n",
    "    \"\"\"\n",
    "    new_df = df.select(['Country', 'AverageTemperature']).groupby('Country').avg()\n",
    "    \n",
    "    new_df = new_df.withColumnRenamed('avg(AverageTemperature)', 'average_temperature')\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function creates a country dimension from the immigration and global land temperatures data.\n",
    "def create_dim_country(df_imm, df_temp, dim_output_data):\n",
    "    \"\"\"This function creates a country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    in: df: spark dataframe of immigration events\n",
    "    in: temp_df: spark dataframe of global land temperatures data.\n",
    "    in: dim_output_data: path to write dimension dataframe to\n",
    "    out: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    #udf that returns the country name\n",
    "    @udf()\n",
    "    def get_country_names(code):\n",
    "        name = mapping_codes[mapping_codes['code']==code]['Name'].iloc[0]\n",
    "        \n",
    "        if name:\n",
    "            return name.title()\n",
    "        return None\n",
    "    \n",
    "    #udf that returns the average temperature for the given country\n",
    "    @udf('string')\n",
    "    def get_country_average_temp(country_name):\n",
    "        print(\"Processing average temperature for: \", country_name)\n",
    "        avg_temperature = df_temp_agg[df_temp_agg['Country']==country_name]['average_temperature']\n",
    "        if not avg_temperature.empty:\n",
    "            return str(avg_temperature.iloc[0])\n",
    "        return None\n",
    "    \n",
    "    #Gets the aggregated temperature data\n",
    "    df_temp_agg = aggregate_temperature_data(df_temp).toPandas()\n",
    "    \n",
    "    #country_codes.csv holds the country codes\n",
    "    file_name = \"country_codes.csv\"\n",
    "    \n",
    "    mapping_codes = pd.read_csv(file_name)\n",
    "        \n",
    "    #Select and renames the i94res column as country_code\n",
    "    dim_country_temp = df_imm.select(['i94res']).distinct() \\\n",
    "                .withColumnRenamed('i94res', 'country_code')\n",
    "    \n",
    "    #Creates the country_name column\n",
    "    dim_country_temp = dim_country_temp.withColumn('country_name', get_country_names(dim_country_temp.country_code))\n",
    "    \n",
    "    #Creates the average_temperature column\n",
    "    dim_country_temp = dim_country_temp.withColumn('average_temperature', get_country_average_temp(dim_country_temp.country_name))\n",
    "    \n",
    "    \n",
    "    integer_cols = ['country_code']\n",
    "    #Type conversion for integer columns\n",
    "    dim_country_temp = cast_type(dim_country_temp, dict(zip(integer_cols, len(integer_cols)*[IntegerType()])))\n",
    "    \n",
    "    float_cols = ['average_temperature']\n",
    "    #Type conversion for float columns  \n",
    "    dim_country_temp = cast_type(dim_country_temp, dict(zip(float_cols, len(float_cols)*[DoubleType()])))\n",
    "    \n",
    "    #Writes the dimension to a parquet file\n",
    "    dim_country_temp.write.parquet(dim_output_data + \"dim_country\", mode=\"overwrite\")\n",
    "\n",
    "    return dim_country_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the df_dim_country dataframe and writing the files to S3\n",
    "df_dim_country = create_dim_country(df_immigration, df_temperature, dim_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_code: integer (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema\n",
    "df_dim_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-------------------+\n",
      "|country_code|country_name|average_temperature|\n",
      "+------------+------------+-------------------+\n",
      "|         692|     Ecuador|      20.5391705374|\n",
      "|         299|    Mongolia|     -3.36548531952|\n",
      "|         576| El Salvador|      25.2628525509|\n",
      "|         735|  Montenegro|      10.2210401137|\n",
      "|         206|   Hong Kong|      21.4236961538|\n",
      "+------------+------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_dim_country.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function creates a visa type dimension from the immigration data.\n",
    "def create_dim_visa_type(df_imm, dim_output_data):\n",
    "    \"\"\"\n",
    "    in: df_imm: immigration dataframe\n",
    "    in: dim_output_data: path to write dimension dataframe to\n",
    "    out: visa type dataframe\n",
    "    \"\"\"\n",
    "    #Creates visatype df from visatype column\n",
    "    visatype_df = df_imm.select(['visatype']).distinct()\n",
    "    \n",
    "    #Adding an id column\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "    \n",
    "    varchar_cols = ['visa_type_key']\n",
    "    #Type conversion for varchar columns\n",
    "    visatype_df = cast_type(visatype_df, dict(zip(varchar_cols, len(varchar_cols)*[StringType()])))\n",
    "    \n",
    "    #Writing dimension to parquet file\n",
    "    visatype_df.write.parquet(dim_output_data + \"dim_visatype\", mode=\"overwrite\")\n",
    "    \n",
    "    return visatype_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the df_dim_visatype dataframe and writing the files to S3\n",
    "df_dim_visatype = create_dim_visa_type(df_immigration, dim_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- visa_type_key: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema\n",
    "df_dim_visatype.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|visatype|visa_type_key|\n",
      "+--------+-------------+\n",
      "|      F2| 103079215104|\n",
      "|     GMB| 352187318272|\n",
      "|      B2| 369367187456|\n",
      "|      F1| 498216206336|\n",
      "|     CPL| 601295421440|\n",
      "+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_dim_visatype.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function creates a us demographics dimension table from the us cities demographics data.\n",
    "def create_dim_demographics(df_demo, dim_output_data):\n",
    "    \"\"\"\n",
    "    in: df_demo: spark dataframe of us demographics survey data\n",
    "    in: dim_output_data: path to write dimension dataframe to\n",
    "    out: spark dataframe representing demographics dimension\n",
    "    \"\"\"\n",
    "    df_demo = df_demo.withColumnRenamed('Median Age','median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population') \\\n",
    "            .withColumnRenamed('Total Population', 'total_population') \\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "\n",
    "    #Adding an id column\n",
    "    df_demo = df_demo.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "\n",
    "    varchar_cols = ['id']\n",
    "    #Type conversion for varchar columns\n",
    "    df_demo = cast_type(df_demo, dict(zip(varchar_cols, len(varchar_cols)*[StringType()])))\n",
    "    \n",
    "    integer_cols = ['male_population','female_population', 'total_population', 'number_of_veterans', 'foreign_born', 'count']\n",
    "    #Type conversion for integer columns\n",
    "    df_demo = cast_type(df_demo, dict(zip(integer_cols, len(integer_cols)*[IntegerType()])))\n",
    "    \n",
    "    float_cols = ['median_age', 'average_household_size']\n",
    "    #Type conversion for float columns\n",
    "    df_demo = cast_type(df_demo, dict(zip(float_cols, len(float_cols)*[DoubleType()])))\n",
    "    \n",
    "    #Writing dimension to parquet file\n",
    "    df_demo.write.parquet(dim_output_data + \"dim_demographics\", mode=\"overwrite\")\n",
    "    \n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the df_dim_demographics dataframe and writing the files to S3\n",
    "df_dim_demographics = create_dim_demographics(df_demographics, dim_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema\n",
    "df_dim_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>number_of_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>state_code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wilmington</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>52346</td>\n",
       "      <td>63601</td>\n",
       "      <td>115947</td>\n",
       "      <td>5908</td>\n",
       "      <td>7401</td>\n",
       "      <td>2.24</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>3152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>Florida</td>\n",
       "      <td>35.3</td>\n",
       "      <td>175517</td>\n",
       "      <td>193511</td>\n",
       "      <td>369028</td>\n",
       "      <td>20636</td>\n",
       "      <td>58795</td>\n",
       "      <td>2.47</td>\n",
       "      <td>FL</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>95154</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gastonia</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>36.9</td>\n",
       "      <td>35527</td>\n",
       "      <td>39023</td>\n",
       "      <td>74550</td>\n",
       "      <td>3537</td>\n",
       "      <td>5715</td>\n",
       "      <td>2.67</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>2788</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>Texas</td>\n",
       "      <td>33.9</td>\n",
       "      <td>50422</td>\n",
       "      <td>53283</td>\n",
       "      <td>103705</td>\n",
       "      <td>4813</td>\n",
       "      <td>8225</td>\n",
       "      <td>2.59</td>\n",
       "      <td>TX</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1057</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City           State  median_age  male_population  female_population  \\\n",
       "0      Quincy   Massachusetts        41.0            44129              49500   \n",
       "1  Wilmington  North Carolina        35.5            52346              63601   \n",
       "2       Tampa         Florida        35.3           175517             193511   \n",
       "3    Gastonia  North Carolina        36.9            35527              39023   \n",
       "4       Tyler           Texas        33.9            50422              53283   \n",
       "\n",
       "   total_population  number_of_veterans  foreign_born  average_household_size  \\\n",
       "0             93629                4147         32935                    2.39   \n",
       "1            115947                5908          7401                    2.24   \n",
       "2            369028               20636         58795                    2.47   \n",
       "3             74550                3537          5715                    2.67   \n",
       "4            103705                4813          8225                    2.59   \n",
       "\n",
       "  state_code                               Race  Count id  \n",
       "0         MA                              White  58723  0  \n",
       "1         NC                              Asian   3152  1  \n",
       "2         FL                 Hispanic or Latino  95154  2  \n",
       "3         NC                              Asian   2788  3  \n",
       "4         TX  American Indian and Alaska Native   1057  4  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_dim_demographics.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This function creates an country dimension from the immigration and global land temperatures data.\n",
    "def create_fact_immigration(spark, df_imm, dim_output_data):\n",
    "    \"\"\"\n",
    "    in: spark: spark session\n",
    "    in: df_imm: spark dataframe of immigration events.\n",
    "    in: output_data: path to write dimension dataframe to\n",
    "    out: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    " \n",
    "    #Gets the dim_visatype_df dimension\n",
    "    dim_visatype_df = spark.read.parquet(dim_output_data + \"dim_visatype\")\n",
    "\n",
    "    #Creates a view for dim_visatype_df dimension\n",
    "    dim_visatype_df.createOrReplaceTempView(\"v_visa\")\n",
    "\n",
    "    #Create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "\n",
    "    #Renames columns to align with data model\n",
    "    df_imm = df_imm.withColumnRenamed('ccid', 'record_id') \\\n",
    "        .withColumnRenamed('i94res', 'country_residence_code') \\\n",
    "        .withColumnRenamed('i94addr', 'state_code')\n",
    "\n",
    "    #Creates an immigration view\n",
    "    df_imm.createOrReplaceTempView(\"v_immigration\")\n",
    "\n",
    "    #Creates the visa_type key\n",
    "    df_imm = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            v_immigration.*, v_visa.visa_type_key\n",
    "        FROM v_immigration\n",
    "        LEFT JOIN v_visa ON v_visa.visatype=v_immigration.visatype\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    #Converts arrival date into datetime object\n",
    "    df_imm = df_imm.withColumn(\"arrdate\", get_datetime(df_imm.arrdate))\n",
    "\n",
    "    #Drop visatype key\n",
    "    df_imm = df_imm.drop(df_imm.visatype)\n",
    "\n",
    "    int_cols = ['cicid','i94yr','i94mon','i94','i94mode','i94bir','i94visa','biryear','admnum']\n",
    "    #Type conversion for integer columns\n",
    "    df_imm = cast_type(df_imm, dict(zip(int_cols, len(int_cols)*[IntegerType()])))\n",
    "    \n",
    "    varchar_cols = ['visa_type_key']\n",
    "    #Type conversion for varchar columns\n",
    "    df_imm = cast_type(df_imm, dict(zip(varchar_cols, len(varchar_cols)*[StringType()])))\n",
    "    \n",
    "    #Writes dimension to parquet file\n",
    "    df_imm.write.parquet(fact_output_data + \"fact_immigration\", mode=\"overwrite\")\n",
    "\n",
    "    return df_imm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Creating the df_fact_immigration dataframe and writing the files to S3\n",
    "df_fact_immigration = create_fact_immigration(spark,df_immigration, dim_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- country_residence_code: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visa_type_key: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the schema\n",
    "df_fact_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>country_residence_code</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>state_code</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visa_type_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>299</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NY</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>M</td>\n",
       "      <td>1962</td>\n",
       "      <td>06292016</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00087</td>\n",
       "      <td>884763262976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>305</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>NY</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>M</td>\n",
       "      <td>1953</td>\n",
       "      <td>06292016</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00087</td>\n",
       "      <td>884763262976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>496</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>IL</td>\n",
       "      <td>20548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>M</td>\n",
       "      <td>1952</td>\n",
       "      <td>06292016</td>\n",
       "      <td>None</td>\n",
       "      <td>OS</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00065</td>\n",
       "      <td>738734374912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>558</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>20547.0</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>M</td>\n",
       "      <td>1974</td>\n",
       "      <td>06292016</td>\n",
       "      <td>M</td>\n",
       "      <td>LH</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00454</td>\n",
       "      <td>738734374912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>596</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>NAS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>FL</td>\n",
       "      <td>20547.0</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>N</td>\n",
       "      <td>M</td>\n",
       "      <td>1992</td>\n",
       "      <td>06292016</td>\n",
       "      <td>M</td>\n",
       "      <td>UP</td>\n",
       "      <td>2147483647</td>\n",
       "      <td>00221</td>\n",
       "      <td>884763262976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid  i94yr  i94mon  i94cit  country_residence_code i94port     arrdate  \\\n",
       "0    299   2016       4   103.0                   103.0     NYC  2016-04-01   \n",
       "1    305   2016       4   103.0                   103.0     NYC  2016-04-01   \n",
       "2    496   2016       4   103.0                   103.0     CHI  2016-04-01   \n",
       "3    558   2016       4   103.0                   103.0     SFR  2016-04-01   \n",
       "4    596   2016       4   103.0                   103.0     NAS  2016-04-01   \n",
       "\n",
       "   i94mode state_code  depdate      ...       entdepa  entdepd  matflag  \\\n",
       "0        1         NY  20550.0      ...             O        O        M   \n",
       "1        1         NY  20555.0      ...             O        O        M   \n",
       "2        1         IL  20548.0      ...             O        O        M   \n",
       "3        1         CA  20547.0      ...             G        O        M   \n",
       "4        1         FL  20547.0      ...             G        N        M   \n",
       "\n",
       "  biryear   dtaddto gender airline      admnum  fltno visa_type_key  \n",
       "0    1962  06292016   None      OS  2147483647  00087  884763262976  \n",
       "1    1953  06292016   None      OS  2147483647  00087  884763262976  \n",
       "2    1952  06292016   None      OS  2147483647  00065  738734374912  \n",
       "3    1974  06292016      M      LH  2147483647  00454  738734374912  \n",
       "4    1992  06292016      M      UP  2147483647  00221  884763262976  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data\n",
    "df_fact_immigration.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    " * Unit tests have been performed in the notebook\n",
    " * Count checks are implemented, exception is thrown if no data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dim_visatype has 17 records. Data quality check successful. \n",
      "df_dim_country has 229 records. Data quality check successful. \n",
      "df_dim_demographics has 2,875 records. Data quality check successful. \n",
      "df_dim_calendar has 30 records. Data quality check successful. \n",
      "df_fact_immigration has 3,096,313 records. Data quality check successful. \n"
     ]
    }
   ],
   "source": [
    "#Checking the number of rows in each dataframe one by one. Throws exception if unloaded table detected.\n",
    "\n",
    "df_check = {\n",
    "    \n",
    "    'df_dim_visatype': df_dim_visatype,\n",
    "    'df_dim_country': df_dim_country,\n",
    "    'df_dim_demographics': df_dim_demographics,\n",
    "    'df_dim_calendar': df_dim_calendar,\n",
    "    'df_fact_immigration': df_fact_immigration,\n",
    "}\n",
    "for df_name, df in df_check.items():\n",
    " \n",
    "    num_of_rows = df.count()\n",
    "\n",
    "    if num_of_rows == 0:\n",
    "        raise ValueError (f\"{df_name} has zero records. Data quality check failed. \")\n",
    "    else:\n",
    "        print(f\"{df_name} has {num_of_rows:,} records. Data quality check successful. \") \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " #### 4.3 Data Dictionary\n",
    " ##### Dim Calendar (Source: Immigration Dataset)\n",
    " \n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Unique id</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival date into US</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">arrival_year</td><td class=\"tg-0pky\">Arrival year into US</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_month</td><td class=\"tg-0pky\">Arrival MonthS</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_day</td><td class=\"tg-0pky\">Arrival Day</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_week</td><td class=\"tg-0pky\">Arrival Week</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_weekday</td><td class=\"tg-0pky\">Arrival WeekDay</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dim Demographics (Source: Demographics Data)\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Record id</td>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state code </td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dim Country (Source: Temperature Data and 94_SAS_Labels_Descriptions.SAS file)\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">country_code</td><td class=\"tg-0pky\">Unique country code</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_name</td><td class=\"tg-0pky\">Name of country</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">average_temperature</td><td class=\"tg-0pky\">Average temperature of country</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dim Visa Type (Source: Immigration Data)\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">Unique id for each visa issued</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type</td><td class=\"tg-0pky\">Name of visa</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Fact Immigration (Source: Immigration Data)\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">record_id</td><td class=\"tg-0pky\">Unique record ID</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_residence_code</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td></tr>    \n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">A numerical key that links to the visa_type dimension table</td></tr>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td></tr>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td></tr>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td></tr>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td></tr>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "* The architecture designed for this project mainly combines most of the technologies covered in the Data Engineering Nano Degree program. Airflow provides the data pipeline orchestration, scheduling utilities as well as the predefined operators give flexibility to process and ingest data. Spark enables us to process greater amounts of data in parallel. S3 is a great storage option for staging massive amounts of data.  Redshift is a scalable datawarehouse platform that would be used for repoting purposes. Developing an end to end pipeline gives the chance to utilize these technologies all at once with great compatibility.\n",
    "\n",
    "* The main data source for this project was the immigration dataset, which was provided in monthly partitions. The flow is scheduled to work once in a month but it can definetely be converted into weekly or daily schedules by adjusting loading mechanism of the source files.\n",
    "\n",
    "* Scenarios:\n",
    "\n",
    " * If the data was increased by 100x I believe this architecture would still perform well. Some performance issues would have probably occured on Spark servers which could be solved easily by some capacity increase. I would also make daily batches instead of running monthly.\n",
    " \n",
    " * If the data populates a dashboard that must be updated on a daily basis by 7am every day, this means we would need daily schedules, that would probably start running after 00:00 AM just after the source files are created.\n",
    " \n",
    " * If the database needed to be accessed by 100+ people, Redshift would still perform well with this Star schema model. Of course some database and query optimizations would improve performance.  Also improvements on server capacites can be considered when query loads are more than the servers can handle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
